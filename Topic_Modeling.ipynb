{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL transformed to HTTPS due to an HSTS policy\n",
      "--2023-03-29 13:01:13--  https://edx.netl.doe.gov/resource/dfd1938a-c890-442b-8341-c59d8fe6ee4d/download\n",
      "Resolving edx.netl.doe.gov (edx.netl.doe.gov)... 204.154.136.32\n",
      "Connecting to edx.netl.doe.gov (edx.netl.doe.gov)|204.154.136.32|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://edx.netl.doe.gov/user/login?came_from=http%3A%2F%2Fedx.netl.doe.gov%2Fresource%2Fdfd1938a-c890-442b-8341-c59d8fe6ee4d%2Fdownload [following]\n",
      "--2023-03-29 13:01:14--  https://edx.netl.doe.gov/user/login?came_from=http%3A%2F%2Fedx.netl.doe.gov%2Fresource%2Fdfd1938a-c890-442b-8341-c59d8fe6ee4d%2Fdownload\n",
      "Reusing existing connection to edx.netl.doe.gov:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5686 (5.6K) [text/html]\n",
      "Saving to: ‘matric_demo_sidcaad2ce9d34900c599a16c2d8b2ba8f1_discoveries_parsed_csv_21.zip’\n",
      "\n",
      "matric_demo_sidcaad 100%[===================>]   5.55K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-03-29 13:01:14 (122 MB/s) - ‘matric_demo_sidcaad2ce9d34900c599a16c2d8b2ba8f1_discoveries_parsed_csv_21.zip’ saved [5686/5686]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need to specify name and path and avoid duplicate downloads\n",
    "!wget -O matric_demo_sidcaad2ce9d34900c599a16c2d8b2ba8f1_discoveries_parsed_csv_21.zip http://edx.netl.doe.gov/resource/dfd1938a-c890-442b-8341-c59d8fe6ee4d/download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the data\n",
    "\n",
    "data = []\n",
    "\n",
    "zf = zipfile.ZipFile('matric_demo_sidcaad2ce9d34900c599a16c2d8b2ba8f1_discoveries_parsed_csv_21.zip')\n",
    "\n",
    "for f in zf.infolist():\n",
    "        try: \n",
    "                zf.open(f.filename)\n",
    "                df = pd.read_csv(zf.open(f.filename), usecols = [\"body\"], header=0)\n",
    "                data.append(df)\n",
    "        except:\n",
    "                continue\n",
    "\n",
    "# total length of list, this number equals total number of products\n",
    "#print(len(data))\n",
    "\n",
    "# first row of the list\n",
    "#print(data[0])\n",
    "\n",
    "discoveries = pd.concat(data, axis=0, ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discoveries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discoveries.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discoveries.body = discoveries.body.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discoveries.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "additional_stop_words = [\"rt\",\"like\",\"amp\",\"i'm\",\"get\",\"u\",\"got\",\"lol\"] \n",
    "stop_words += additional_stop_words\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def preprocess(text):\n",
    "  # Make all text lower case\n",
    "  text = text.lower()\n",
    " \n",
    "  #Removing special character\n",
    "  text = re.sub('[!\"”$%&’()*+,-./:;<=>?[\\]^_`{|}~@#]','',text)\n",
    "  \n",
    "  #text = [word for word in text if len(word)>2]\n",
    "\n",
    "  # Tokenize text\n",
    "  text = word_tokenize(text)\n",
    "  \n",
    "  # Lemmatize text\n",
    "  text = [lemmatizer.lemmatize(word) for word in text] \n",
    "  \n",
    "  # Remove stopwords from text\n",
    "  text = [word for word in text if word not in stop_words]\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import collections\n",
    "\n",
    "import itertools\n",
    "\n",
    "def preprocess_ngrams(text, index):\n",
    "    from nltk.util import ngrams\n",
    "    text = text.lower()\n",
    "    text = re.sub('[!\"”$%&’()*+,-./:;<=>?[\\]^_`{|}~@#]','',text)\n",
    "    text = word_tokenize(text)\n",
    "    terms_ngrams = [list(ngrams(text, index))]\n",
    "\n",
    "    terms_ngrams = [ngram for ngram in terms_ngrams if ngram[0] not in stop_words and ngram[index-1] not in stop_words]\n",
    "   \n",
    "    grams = list(itertools.chain(*terms_ngrams))\n",
    "\n",
    "    # ngram_counts = collections.Counter(grams)\n",
    "    # ngram_counts.most_common(10)\n",
    "    # ngram_df = pd.DataFrame(ngram_counts.most_common(10))\n",
    "    return grams\n",
    "    # print(\"\\nNgram results: \")f\n",
    "    # display(ngram_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discoveries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discoveries[\"clean_body\"] = discoveries.body.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is freezing before completing one iteration\n",
    "n = 2\n",
    "for i in range(1, n+1):\n",
    "    discoveries[str(i) + \"-grams\"] = discoveries.body.apply(preprocess_ngrams,index = i)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discoveries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = ['overall', 'body']\n",
    "# if n>=1:\n",
    "#     df_columns.append(\"clean_body\")\n",
    "# if n>=2:\n",
    "#     df_columns.append(\"digrams\")\n",
    "# if n>=3:\n",
    "#     df_columns.append(\"trigrams\")\n",
    "# if n>=4:\n",
    "for i in range(1, n+1):\n",
    "    df_columns.append(str(i) + \"-grams\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(discoveries, columns=['overall', 'body', 'clean_body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = dict()\n",
    "\n",
    "for text in df['clean_body']:\n",
    "  for word in text:\n",
    "    if word in word_count: \n",
    "      word_count[word] += 1\n",
    "    else:\n",
    "      word_count[word] = 1\n",
    "\n",
    "sorted(word_count.items(),key = lambda x: x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq pyLDAvis\n",
    "!pip install -qq gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary \n",
    "from gensim.models import LdaMulticore\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(discoveries['clean_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out low-frequency / high-frequency, also limit the vocabulary to max 1000 words\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in discoveries['clean_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "lda = LdaMulticore(corpus, id2word=dictionary, num_topics=4, workers=3, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the topics\n",
    "lda.print_topics(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "lda_vis = pyLDAvis.gensim.prepare(lda, corpus, dictionary) \n",
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda[corpus][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lda[corpus][0])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discoveries['topic'] = [sorted(lda[corpus][text])[0][0] for text in range(len(discoveries['clean_body']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discoveries.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Spacy's pre-trained model (english version):\n",
    "!python -m spacy download en_core_web_md -qq\n",
    "\n",
    "#The code snippet above installs the larger-than-standard en_core_web_md library, \n",
    "#which includes 20k unique vectors with 300 dimensions.\n",
    "#source: https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_md\n",
    "nlp2 = en_core_web_md.load(disable=[\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "doc1 = nlp2(discoveries.iloc[0][\"body\"])\n",
    "doc2 = nlp2(discoveries.iloc[1][\"body\"])\n",
    "doc_list = []\n",
    "print(discoveries.iloc[0][\"clean_body\"])\n",
    "for sent in discoveries[\"clean_body\"]:\n",
    "    doc = Doc(nlp2.vocab, sent)\n",
    "    doc_list.append(nlp2(doc))\n",
    "print(doc_list[0].similarity(doc_list[1]))\n",
    "print(discoveries.iloc[0][\"clean_body\"])\n",
    "print(doc2)\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_embedding_dic = {}\n",
    "\n",
    "for l in tokens:\n",
    "    for word in l:\n",
    "        spacy_embedding_dic[word] = nlp2(word).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_embedding_dic['fl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_embedding = pd.DataFrame.from_dict(spacy_embedding_dic)\n",
    "s_embedding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_spacy = reducer.fit_transform(s_embedding)\n",
    "umap_spacy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = umap.UMAP().fit(s_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# and instantiate it where we need to specify that we want it to create 3 clusters (three types of penguins)\n",
    "clusterer = KMeans(n_clusters=4)\n",
    "clusterer.fit(s_embedding) #we only fit here, since no data needs to be transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(umap_spacy[:,0], umap_spacy[:,1], hue = clusterer.labels_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
